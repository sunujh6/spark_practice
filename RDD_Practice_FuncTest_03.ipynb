{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/big-edu/.local/lib/python3.7/site-packages/IPython/utils/py3compat.py:188 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3076f1b26757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RDDParctice_03\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[4]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /home/big-edu/.local/lib/python3.7/site-packages/IPython/utils/py3compat.py:188 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"RDDParctice_03\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    def myFunc(s):\n",
    "        words = s.split(\" \")\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_lengths=sc.textFile(\"../README.md\").flatMap(myFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Apache', 'Spark', '', 'Spark', 'is', 'a', 'fast', 'and', 'general', 'cluster', 'computing', 'system', 'for', 'Big', 'Data.', 'It', 'provides', 'high-level', 'APIs', 'in', 'Scala,', 'Java,', 'Python,', 'and', 'R,', 'and', 'an', 'optimized', 'engine', 'that', 'supports', 'general', 'computation', 'graphs', 'for', 'data', 'analysis.', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'Spark', 'SQL', 'for', 'SQL', 'and', 'DataFrames,', 'MLlib', 'for', 'machine', 'learning,', 'GraphX', 'for', 'graph', 'processing,', 'and', 'Spark', 'Streaming', 'for', 'stream', 'processing.', '', '<http://spark.apache.org/>', '', '', '##', 'Online', 'Documentation', '', 'You', 'can', 'find', 'the', 'latest', 'Spark', 'documentation,', 'including', 'a', 'programming', 'guide,', 'on', 'the', '[project', 'web', 'page](http://spark.apache.org/documentation.html).', 'This', 'README', 'file', 'only', 'contains', 'basic', 'setup', 'instructions.', '', '##', 'Building', 'Spark', '', 'Spark', 'is', 'built', 'using', '[Apache', 'Maven](http://maven.apache.org/).', 'To', 'build', 'Spark', 'and', 'its', 'example', 'programs,', 'run:', '', '', '', '', '', 'build/mvn', '-DskipTests', 'clean', 'package', '', '(You', 'do', 'not', 'need', 'to', 'do', 'this', 'if', 'you', 'downloaded', 'a', 'pre-built', 'package.)', '', 'You', 'can', 'build', 'Spark', 'using', 'more', 'than', 'one', 'thread', 'by', 'using', 'the', '-T', 'option', 'with', 'Maven,', 'see', '[\"Parallel', 'builds', 'in', 'Maven', '3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).', 'More', 'detailed', 'documentation', 'is', 'available', 'from', 'the', 'project', 'site,', 'at', '[\"Building', 'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', '', 'For', 'general', 'development', 'tips,', 'including', 'info', 'on', 'developing', 'Spark', 'using', 'an', 'IDE,', 'see', '[\"Useful', 'Developer', 'Tools\"](http://spark.apache.org/developer-tools.html).', '', '##', 'Interactive', 'Scala', 'Shell', '', 'The', 'easiest', 'way', 'to', 'start', 'using', 'Spark', 'is', 'through', 'the', 'Scala', 'shell:', '', '', '', '', '', './bin/spark-shell', '', 'Try', 'the', 'following', 'command,', 'which', 'should', 'return', '1000:', '', '', '', '', '', 'scala>', 'sc.parallelize(1', 'to', '1000).count()', '', '##', 'Interactive', 'Python', 'Shell', '', 'Alternatively,', 'if', 'you', 'prefer', 'Python,', 'you', 'can', 'use', 'the', 'Python', 'shell:', '', '', '', '', '', './bin/pyspark', '', 'And', 'run', 'the', 'following', 'command,', 'which', 'should', 'also', 'return', '1000:', '', '', '', '', '', '>>>', 'sc.parallelize(range(1000)).count()', '', '##', 'Example', 'Programs', '', 'Spark', 'also', 'comes', 'with', 'several', 'sample', 'programs', 'in', 'the', '`examples`', 'directory.', 'To', 'run', 'one', 'of', 'them,', 'use', '`./bin/run-example', '<class>', '[params]`.', 'For', 'example:', '', '', '', '', '', './bin/run-example', 'SparkPi', '', 'will', 'run', 'the', 'Pi', 'example', 'locally.', '', 'You', 'can', 'set', 'the', 'MASTER', 'environment', 'variable', 'when', 'running', 'examples', 'to', 'submit', 'examples', 'to', 'a', 'cluster.', 'This', 'can', 'be', 'a', 'mesos://', 'or', 'spark://', 'URL,', '\"yarn\"', 'to', 'run', 'on', 'YARN,', 'and', '\"local\"', 'to', 'run', 'locally', 'with', 'one', 'thread,', 'or', '\"local[N]\"', 'to', 'run', 'locally', 'with', 'N', 'threads.', 'You', 'can', 'also', 'use', 'an', 'abbreviated', 'class', 'name', 'if', 'the', 'class', 'is', 'in', 'the', '`examples`', 'package.', 'For', 'instance:', '', '', '', '', '', 'MASTER=spark://host:7077', './bin/run-example', 'SparkPi', '', 'Many', 'of', 'the', 'example', 'programs', 'print', 'usage', 'help', 'if', 'no', 'params', 'are', 'given.', '', '##', 'Running', 'Tests', '', 'Testing', 'first', 'requires', '[building', 'Spark](#building-spark).', 'Once', 'Spark', 'is', 'built,', 'tests', 'can', 'be', 'run', 'using:', '', '', '', '', '', './dev/run-tests', '', 'Please', 'see', 'the', 'guidance', 'on', 'how', 'to', '[run', 'tests', 'for', 'a', 'module,', 'or', 'individual', 'tests](http://spark.apache.org/developer-tools.html#individual-tests).', '', 'There', 'is', 'also', 'a', 'Kubernetes', 'integration', 'test,', 'see', 'resource-managers/kubernetes/integration-tests/README.md', '', '##', 'A', 'Note', 'About', 'Hadoop', 'Versions', '', 'Spark', 'uses', 'the', 'Hadoop', 'core', 'library', 'to', 'talk', 'to', 'HDFS', 'and', 'other', 'Hadoop-supported', 'storage', 'systems.', 'Because', 'the', 'protocols', 'have', 'changed', 'in', 'different', 'versions', 'of', 'Hadoop,', 'you', 'must', 'build', 'Spark', 'against', 'the', 'same', 'version', 'that', 'your', 'cluster', 'runs.', '', 'Please', 'refer', 'to', 'the', 'build', 'documentation', 'at', '[\"Specifying', 'the', 'Hadoop', 'Version', 'and', 'Enabling', 'YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)', 'for', 'detailed', 'guidance', 'on', 'building', 'for', 'a', 'particular', 'distribution', 'of', 'Hadoop,', 'including', 'building', 'for', 'particular', 'Hive', 'and', 'Hive', 'Thriftserver', 'distributions.', '', '##', 'Configuration', '', 'Please', 'refer', 'to', 'the', '[Configuration', 'Guide](http://spark.apache.org/docs/latest/configuration.html)', 'in', 'the', 'online', 'documentation', 'for', 'an', 'overview', 'on', 'how', 'to', 'configure', 'Spark.', '', '##', 'Contributing', '', 'Please', 'review', 'the', '[Contribution', 'to', 'Spark', 'guide](http://spark.apache.org/contributing.html)', 'for', 'information', 'on', 'how', 'to', 'get', 'started', 'contributing', 'to', 'the', 'project.']\n"
     ]
    }
   ],
   "source": [
    "print(tell_lengths.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
